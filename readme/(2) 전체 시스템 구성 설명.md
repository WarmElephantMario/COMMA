# (2) 전체 시스템 구성

COMMA는 장애인 사용자의 강의 자료 접근성을 높이기 위해 AI 및 음성 인식 기술을 활용하여 실시간 자막 및 대체텍스트 생성을 지원하는 안드로이드 어플리케이션이다. COMMA는 프**론트엔드와 백엔드 파트**로 나뉘어 구성된다. **프론트엔드**에서는 장애인 친화적인 사용자 인터페이스를 제공하고, **백엔드**에서는 AI 모델을 활용하여 텍스트 생성 기능을 수행한 후, 처리된 정보를 **데이터베이스**에 저장한다.

![sw구조](./SW구조.png)
<br><br>

## 1. 주요 모듈
### (1) 프론트엔드

프론트엔드는 Android 플랫폼을 기반으로 Flutter와 Dart를 활용하여 개발되었다. 
프론트엔드는 장애인 친화적인 사용자 인터페이스를 제공하며, 텍스트 크기 조절, 화면 밝기 보드 전환 등의 접근성 기능을 포함하고 있다.

- **기술 튜닝 방법**: Flutter의 `MediaQuery`와 `Provider`를 이용해 사용자 설정에 따라 동적으로 텍스트 크기, 화면 밝기 모드와 색상 조절할 수 있도록 하였다.
- **기술 튜닝 결과**: 국제 웹 접근성 지침은 저시력 사용자를 위해 폰트 크기 변경, 화면 밝기 모드 전환 등의 접근성 기능을 포함하도록 권고하고 있다. COMMA는 당 규정에 근거하여, 저시력 및 시각장애 사용자의 디지털 접근성을 증진하였다.

### (2) 백엔드

Node.js를 통해 구축된 COMMA의 서버는 AWS EC2 인스턴스에 배포되어 24시간 구동 중이다. 서버는 프론트엔드로부터 받은 강의 자료를 AI 모델로 전달해 대체 텍스트를 생성하고 핵심 키워드를 추출한다. 서버는 이때 생성된 데이터를 데이터베이스에 저장하고, 필요 시 다시 불러올 수 있도록 지원한다.
- **기술 튜닝 방법**: Node.js의 `Promise`와 Dart의 `Future`를 활용해 비동기 처리를 구현하여 데이터가 더 빠르게 처리되도록 하였다. 요청량이 많아져도 비동기 처리를 통해 각 요청을 효율적으로 처리하여 서버의 응답 속도를 개선하였다.
- **튜닝 결과**: 10페이지 이내의 강의 자료는 1분 이내, 20페이지 이내의 강의 자료는 1분 20초 이내, 30페이지 이내의 강의 자료는 1분 40초 이내로 인공지능 학습이 완료됨을 확인하였다. 장애인 학우를 대상으로 한 베타테스트 결과, ‘강의 자료의 인공지능 학습 소요 시간’에 대한 만족도가 ‘매우 만족’과 ‘만족’의 비율이 73%로 드러났다.

### (3) 데이터베이스

데이터베이스는  MySQL과 Firebase를 결합하여 구축하였다. 데이터베이스는 사용자가 업로드한 강의 자료, 생성된 대체 텍스트, 실시간 자막 파일 등을 저장한다.
        
- **기술 튜닝 방법**: 용량이 크고 자주 접근하는 데이터는 Firebase에 캐싱하고, 기본 데이터는 MySQL에 저장하여 데이터 로드에 소요되는 시간을 단축하였다.
- **튜닝 결과**: 10페이지 이내의 강의 자료를 로드하는 데에 10초 미만의 시간이 소요된다. 20페이지 이내는 약 14초, 30페이지 이내의 자료는 약 20초가 소요된다. 강의 자료의 모든 페이지를 불러오는 데 소요되는 시간이 매우 짧아 사용자에게 큰 불편을 주지 않을 것으로 예상한다.

### (4) OpenAI (AI)

OpenAI 모델은 강의 자료에서 대체 텍스트와 핵심 키워드를 추출하는 데 사용된다. 생성된 대체 텍스트를 통해 시각 장애 사용자는 강의 자료의 핵심 내용을 음성으로 들어 이해할 수 있다. 강의 자료에서 추출된 핵심 키워드는 실시간 자막 생성 시 보다 정확하게 인식되어 청각장애 사용자가 실시간 수업에 참여할 때 편의를 돕는다. 
        
- **기술 튜닝 방법**: OpenAI API에 전달되는 Prompt를 튜닝하여 일정한 규격에 맞는 대체 텍스트를 생성하도록 설정하였다. 또한, Prompt에 구체적인 예시와 자세한 지령을 포함하여 학습 자료의 문맥에 맞는 키워드를 추출하도록 설정하였다.
- **튜닝 결과**: 총 2명의 교내 시각장애 학우에게 베타테스트를 시행한 결과, ‘생성되는 대체텍스트를 통해 강의 자료의 내용을 이해할 수 있다’는 데 90%가 동의하였고, ‘생성되는 대체텍스트의 규격이 일정하며 이해하기 쉽다’는 데 88%가 매우 동의하였다. 한국장애인협회의 시각장애 전문가에게 자문을 받은 결과, 대체 텍스트의 품질이 일관적이고 정보 전달력이 높다는 피드백을 받았다.

### (5) Deepgram
        
Deepgram의 Speech-to-Text API는 강의 중 음성을 실시간으로 텍스트로 변환하여 자막을 생성한다. 청각 장애 사용자가 강의 내용을 실시간으로 텍스트로 확인할 수 있도록 지원한다.

- **기술 튜닝 방법**: OpenAI를 통해 강의 자료에서 추출한 핵심 단어를 Deepgram API의 키워드 부스팅 함수에 쿼리로 전달한다. 전달된 단어들은 실시간 자막 생성 시 보다 정확하게 인식되어 청각장애 사용자가 실시간 수업에 참여할 때 편의를 돕는다. 또한, 녹음이 종료된 후 한 차례 더 부정확하게 인식된 키워드 단어들을 올바르게 교정하는 Post-Processing 절차를 추가하여 정확도를 높였다.
- **튜닝 결과**: Deepgram API의 기본 오류율(CER)은 11.957%였으나, 키워드 부스팅을 적용한 후에 오류율(CER)이 10.870%로 감소하였다. Post-processing을 거친 후에는 오류율이 9.78% 추가로 감소하여, 평균 오류율이 10% 대 초반으로 측정되었다. 이는 30분 수업 속기 파일 5,000자를 기준으로 오류 글자 수가 239자에서 217자로 줄어든 것으로, 총 28자 감소한 수치이다.

<br><br>


## 2. **SW 구조에서의 데이터 흐름**
1. **학습 시작하기** : 사용자가 학습 유형을 선택한 후 강의 자료를 업로드하면, 이 자료는 서버로 전송된다. 서버는 사용자가 입력한 정보를 기반으로 데이터베이스에 강의 자료를 저장한다. (강의 폴더 및 강의 파일 생성)
2. **강의 자료 AI 학습 시작** : 사용자가 업로드한 강의 자료의 AI 학습을 시작하면, 서버는 OpenAI에 해당 강의 자료를 보내 대체 텍스트와 핵심 키워드를 생성하도록 요청한다.
3. **대체텍스트, 키워드 저장** : 강의 자료의 AI 학습이 완성되면, OpenAI 모델이 생성한 대체 텍스트와 키워드는 서버를 통해 데이터베이스에 저장된다.
4. **키워드 전달** : OpenAI가 강의 자료에서 추출한 키워드는 실시간 자막 생성 모델인 Deepgram의 키워드 부스팅 함수의 인자로 전달된다. 사용자가 실시간 수업 환경에서 녹음을 시작하면, 서버는 실시간 자막 생성 기능을 위해 Deepgram에 오디오 데이터를 전송하여 음성을 텍스트로 변환하도록 요청한다.
5. **자막 스크립트 저장** : Deepgram을 통해 생성된 자막은 서버를 통해 데이터베이스에 저장된다. (강의 전사 파일)
6. **자막 스크립트, 강의 자료 로드** : 사용자가 콜론 파일 생성을 요청할 시, 데이터베이스에 저장된 자막 스크립트와 강의 자료를 로드하여 서버로 가져온다. 불러온 두 파일을 OpenAI 모델에 전달하여, 강의 자료의 페이지별로 자막 스크립트를 분리하도록 요청한다.
7. **콜론 파일 저장** : 강의 자료의 페이지별로 자막이 분리된 콜론 파일의 생성이 완료되면, 서버를 통해 생성된 콜론 파일을 데이터베이스에 저장한다.

